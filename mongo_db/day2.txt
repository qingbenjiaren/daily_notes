MongoDB数据稳定性
	MongoDB丢失数据的问题
		如何解决
			恢复日志（journaling）
				类似MySQL的RedoLog作用。
			
			写关注（write concern）
				{w:0}:
				{w:1}:
				{w:2}:majority写入大多数节点（推荐）（对于复制集来说）
				
				配置方法
				{w:2} majority
				{j:1} journaled(默认)
				
MongoDB的适用和不适用场景
	适用场景
		更高的写入负载
			默认情况下，MongoDB更侧重搞数据写入性能，而非事务安全，MongoDB很适合业务系统中有大量“低价值”数据的场景。
			但是应当避免在高事务安全
			日志、简历、帖子
		
		高可用性
		
		数据量很大或者未来会变得更大
			在MySQL中，当一个单标达到5-10G时会出现明显性能降级
		
		基于位置的处理
		
		表结构不明确，且数据在不断变大
		
		
		没有DBA支持
		
		
	不适用场景
		重要数据，事务性比较强的，一致性比较强的
		
		
	如果说要存数据，如果不适用mysql推荐使用mongodb
	
	MongoDB文档设计
		文档中的key，禁止使用_外的特殊字符
		
		_id的生成（建议手动生成）
			用业务ID去替代
			在分布式环境中可以用雪花算法
			leaf
		
		Collection结构设计，优先使用内嵌，内嵌不行了再用关联
			在同一个collection中，内嵌文档
			
			单个文档大于16M的时候，Collection关联，引用
					应该避免关联
			
			日志和场景
			
	MongoDB再项目中遇到的问题
		大量删除数据
			问题北京：删除离线IM消息，已读取、未做物理删除，需要删除已读的历史数据
			
			db.message.remove({"flag":1})
			
			产生的问题：造成全表扫描，长时间没执行完成，造成业务无法响应
			
			原因是flag不是索引，造成全表扫描
			
			解决方案：
				kill掉正在执行的op
				长期方案：
					1、在业务层已读的数据直接做物理删除
					2、现在数据做离线删除，在从库导出flag=1的数据，该数据是由主键的（_id），通过脚本根据主键在主库进行批量删除，后从库同步主库数据
					
					在低峰期（0点-6点）执行脚本并控制删除速度
					
				大量数据空洞（碎片，接着问题1而来）
					
					
					解决方案：	
						1、在线压缩数据（Online Compress）
						
						2、收缩数据库
							原理：从库复制主库时，不会复制空数据
								1、停掉从库的进程
								2、删除从库的数据
								3、从库从主库同步数据
								4、把从库提升为主库，对主库做降级
								5、把原来的主库停掉
								6、删除原来主库的数据
								7、启动原来的主库，并从新的主库同步数据
								
					问题：单点操作有风险，最好做一柱两从或多从
					从库同步主库数据，会造成主库性能下降
					


大厂面试题解析
	存储引擎的InnoDB与MyISAM的区别，优缺点，使用场景？
		从各方面去分析：
			存储文件
			事务
			索引
			count
			外键
	说说MySQL优化知道？
		硬件优化
			用profile分析硬件支撑情况
		配置文件优化
			运行较慢的SQL优化，索引，explain，分析type extra等值看sql的索引使用情况，应对索引失效的情况
			慢查询日志
			日志的开关
			降低磁盘写入次数，跳转redolog，checkpoint
		表结构优化（拆表）
		SQL语句优化，索引，limit截断（深分页），小结果集关联大结果集
		分区分表
	
	UndoLog和RedoLog的区别和联系
		Mysql实现ACID：
			undo保证什么：多版本并发控制，回滚，MVCC
			redo保证什么：一致性，原子性，持久性
			undo的完整性也依靠redo
			
		
	Mysql索引的数据结构，及为什么使用这种数据结构。adaptive hash 
		B+tree
		数据在叶子节点上
		
	索引失效的场景：
		顺口溜，总结
		全职匹配
		组合索引
		函数
		like
		select *
		is null is not null
		字符串不加引号，导致的内部类型转换
		
	什么是死锁和死锁的排查和解决
		两个线程互相等待，
			加锁的顺序
			加锁的范围
			死锁原理分析
	innodb的事务与日志的实现方式
		A
		C
		I
		D
		redoLog,undoLog
		
	RC,RR的实现原理及流程
		undolog，readview，事务链表
		MVCC
	什么是MVCC，有什么好处，如何实现
	
	场景题：
		一个6亿的表a，一个3亿的表b，通过外键tid关联，你何如最快速的查询出满足条件的第50000000到底5000200中的这200数据
		
			用where id > 5000000 limit 200
			
			小结果集关联大结果集
			select *from b,(select tid from a limit 500000,200)a where b.id = a.id
		
	分库分表带来的分布式困境与应对之策
		shardingjdbc
		mycat
		直接分到极限（256张表），尽量不要做数据迁移，因为硬件便宜
		一个表少存些数据，有钱，钱能解决的问题就不是大问题，真有那么多数据量的话，公司的业务肯定非常优秀，不差钱
		分库，尽量把相同的业务分到一个库，尽量不要夸库join，所以应该先分库，再分表
		
	主从延迟后，想办法读主就行，mycat可以直接配，sharding需要手写，更新操作，直接返回，缓存
	
	MySQL的高可用

Redis的数据结构
	链表
	字典
	简单动态字符串
	跳表
	
如何提高链表的查询性能
	链表的基础上，再加一个跳表

lua如何解决redis的并发问题

为什么redis是单线程还效率高
	数据结构
	IO多路复用
	持久化（RDB,AOF）
	子进程带数据
Redis分布式锁原理，优势劣势和使用场景

Redis主从，哨兵，集群区别，哪个更好，为什么
	redis-cluster最好，因为不需要哨兵，如果主从加哨兵，哨兵挂了，就不好了
Redis全量同步，增量同步

Redis缓存穿透
Redis缓存击穿
缓存雪崩（大量的KEY过期）

数据写
	保证数据最终一致
	延时双删
	为什么两秒后要删一次，因为在更新的时候有人在并发的读

场景题：	
	Mysql里有2000万数据，redis只存20万的数据，如何保证redis中的数据都是热点数据
		LRU，最近最少使用
			用链表实现的


Mysql和Mongodb的区别
		表
		记录
		事务
		join
		索引

MongoDB存储特性与内部原理？

副本集有几种，区别，有仲裁，无仲裁

副本集和主从的区别

ObjectID的组成

MongoDB的索引是什么，如何使用？

MongoDB中分片是如何实现的

什么是MongoDB的混合集群，有什么优势 三高 （高可用（主从），高扩展（分片），高性能（读写分离））


年后马上是谁，后面也好有的放矢






MongoDB中索引是什么？如何使用
	
		
		设计
		架构
		项目经理
		创业
		兼职
		
		
		
			
			
				
			
				
				
			
		
		
	