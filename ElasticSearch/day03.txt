今日课程内容说明：
	索引分片原理
	横向扩容-继续扩容
	故障转移
	索引库存储
	
	es集群选举
	es近实时搜索
	数据恢复
	故障探查
	脑裂现象（如何避免脑裂的现象）
	文档路由原理
		几个实战中的问题
			多大规模集群
			设置多少个分片
			设置多少个副本分片
			
			
一：
	索引分片的原理
		es存储特点，为了分担服务器压力，提供读写吞吐量，采用分片方式进行存储
		1、ES集群主分片的数量一旦确定就不可更改
		2、副本分片数量理论上是可以无限制扩容，但是真实情况需要考虑master节点维护副本分片的能力问题，如果副本分片太多，也会导致性能下降
		3、一个文档只能存储在一个分片上，不能被分散存储
		4、主分片的数量确定了存储的容量
		5、一个主分片大小理论上是无限制的，一个分片太大的话，在数据读取时，故障转移的时候，就会出现性能下降，一般情况下一个分片最大为50G
		6、极限分片，一个机器存储一个分片
		7、副本分片的作用？数据备份，防止服务器故障引起的数据丢失；提供读操作分担请求压力
		8、一个分片就是一个完整的索引库。提供完整的搜索功能。
	
	集群 横向扩展
		并发访问量越来越大：加机器，扩展副本分片，提高
		数据量越来越大
			每个分片数据达到50G，性能最佳，以太机器可以存储很多分片
			如果数据量越来越大，只能增加主分片的数量（但是，主分片的数量一旦设定就不能更改）
				解决方案是，重新创建索引库，设置与预期的分片数量相同的分片，迁移索引数据即可
		
		案例：
			2个primary shard 2个replica shard
			2台服务器
			
			若并发访问量大怎么应对
			数据存不下怎么应对
		
		注意提供脚本监控
	
	作为架构师：公司的数据规模，分片数量是由架构师主观去设置的，在做索引库构建的时候，就应该考虑这个问题，考虑现有数据，考虑增量数据，考虑并发访问量
	数据规模，预期的数据增量
	
	
	es故障转移问题：
		案例场景：当机器故障、宕机，需要进行故障转移（把分片数据转移到健康的节点上面，继续对外提供服务）
			自动进行的，当有新机器加入集群时，会自动进行负载均衡
			
			注意节点和分片的区别
			分片的空间不要设置太大
			注意一个概念，节点宕机，并不代表分片挂了。ES集群可以做故障转移，把分片转移到集群中其他节点上
			
	es索引存储
		问题：如何保证索引数据不丢失？？
		索引存储结构
			segment
			
			primary shard: segment segment segment   ====索引为什么要分段存储？
				Elastic search索引库有一个约定：（约定大于配置）
					索引和文档一旦写入磁盘就不可更改？？？？
						主要是基于以下几点的考虑：
							数据无需改变，防止并发修改，不需要加锁，不需要担心多个线程同时写索引库。保护索引库数据安全性
							防止索引，索引库被频繁修改写入，浪费性能，占用IO资源
							把索引读入内存，索引不能被改变，索引数据会一直储存在内存，搜索的时候只需要从内存进行搜索，只需要命中内存索引即可，提升性能
							
							问题：增删改应该怎么办？如何解决这个问题
								这个问题就和这个结构（段的结构）有关系了
								答案是：为了不影响原来的索引库，我们会重新创建一个段，每次新增或新增文档都会新增一个段（segment）这样就不会影响原来的索引库。
								不会修改原来的索引库，只会添加一个段，这样的话，对原来的索引库没有影响
								
								commit point文件维护了所有的段
									当要新增或修改数据的时候，会先把数据写入memory buffer内存缓冲区
									然后创建一个新的segment，然后把数据添加到新的段中，此时segment还处于内存中，然后提交之后，这个segment就可以被搜索了、
									所以，在新增的的时候，不会改变原来索引库的结构
									
									更新和删除怎么办
										删除：删除的是文档，索引不删除
											1、在commit point文件中会有一个文件，.del，这个文件中会记录被删除的文档
											2、实际上文档并没有被删除，此文档还可以参与搜索
											3、搜索的结果会根据.del文件进行比对，发现此文档被删除，会把搜索结果从文档中剔除
										
										更新：先删除，再添加
											第一步：走删除流程
											第二步：走新增流程
											
									添加和更新每次都会创建一个新的segment，时间长了，segment就会多到爆炸？？如何解决这个问题？
										应该怎么解决：
											segment合并
											
											怎么合并：？
												将老的段合并到新的段里，
													1、会根据.del删除相应的文档
													2、把旧的segment合并到新的segment
													3、合并完毕后，删除旧的segment
													4、把合并后的新的segment重新提交
													此时用户搜索就会搜索新的段
													
									如何保证索引库的索引不丢失呢？
											translog，WAL机制？
											写入之前先写日志
											
											每隔30分钟刷盘
											当服务器宕机，故障，重启恢复后，数据从translog文件进行恢复，防止数据丢失
											
											为什么每隔30分钟刷盘
											防止磁盘散列存储、
											防止IO开销
											
											
							集群选举：
								如果同时启动，按照nodeid进行排序，取出最小的做master
								
								
								新节点加入：
									节点完成选择后，新节点加入，会发送join request到master节点
									
							节点类型：
								master
								data
								协调
								
						数据恢复
							
							
						故障探查	
							ES有两种集群故障探查机制
								通过master进行的，master会ping集群中所有其他node
								每隔node都会去ping master来确保master或者
								
						
						脑裂现象
							由于网络抖动，master节点和其他node节点无法进行连接，导致节点将会进行重新选举
							此时就会产生脑裂，所谓的脑裂就是存在多个master
							
							如何解决脑裂的问题：
								公式 = (master候选节点+1)/2 == 值 防止脑裂的一个值
								脑裂双方（失联双方群体）
									if(nodeNum >= 值){
										可以选举一个老大
									}else{
										无法选举，甚至会取消master的资格
									}
							通过以上方式，防止脑裂产生
							
							
				集群路由：
					文档路由原理：
						实现文档CRUD，是否知道把文档CRUD到哪个分片？？
						
						ES的路由算法：
							公式： shard = hash(routing)%number_of_primary_shards
							
							shard：计算出的分片的额编号
							hash：提供了一个hash算法
							routing：
								1）_id文档id，默认 routing = _id
								2）自定义的值/user/doc/1?routing=xxx
								number_of_primary_shard：主分片的数量：
								
									一个用户想要添加一个数据
										随机连上一个node进行CRUD
										当请求发送到这个节点后，此节点会自动转变身份，编程协调节点
										协调节点开始使用公式进行路由计算 shard = hash(routing)%number_of_primary_shards,
											/user/doc/1?routing=66 假设hash(66) = 31     31%3 = 1
										计算的结果是把数据向P1这个分片中写入，此时协调节点将会转发请求
										
										等写到primary节点后，primary节点同步到replica节点后，同步就算完成
										
										问题：主分片数量一旦确定就不能更改了？
											是因为，若更改了，同样的算法就定位不到同一个分片了，看公式，因为主分片数量变了，同样的ID过来，计算出的结果就不一样了
											一旦主分片数量变化了，取模的结果就发生变化了
										
						
			几个实战中的问题
				1、多大规模集群？？
				2、设置多少个分片？？
				3、设置多少个副本分片？？
				
				思路：数据量是多大
					  数据增量
					  服务器配置（x86 32G 2T）
				
				思考：
					1、elasticsearch head内存：最大堆内存设置：30G（可以让数据放入内存的大小）
					2、单个分片最大可以存储多少数据，一般情况：30G---50G，如果超过50G大小，查询或写入性能就会急剧下降
				
				推断
					数据规模：10TB级别数据，分析需要几台服务器，多少个主分片？？
					1shard = 50G
					10TB/50G = 205 shard（primary shard）,另外还要至少加上205个replica shard
					10TB / 2TB = 5 台服务器
					
					其实400多分片还是比较少的
					
					elasticsearch秘密
						elasticsearch堆内存可以设置：30G
						每1个G的堆内存支持多少分片：支持20到25个分片
						设置30G的堆内存大小：实际上单机节点可以支持600-750个分片
						
					需要设置多少个副本分片
						参考横向扩容：
							关键看并发量
							
							
项目中讲：ES API使用，业务开发，elk

下节课讲MQ
					
					
					

			
								
			
		
		